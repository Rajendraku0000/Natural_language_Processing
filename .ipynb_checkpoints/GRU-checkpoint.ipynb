{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OP_x1kuAYRb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L792MJ9zAviR"
   },
   "source": [
    "Why we use GRU  whenver we have traditional approaches ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7NlsFgUBQdw"
   },
   "source": [
    "Gated Recurrent Units (GRUs) and other recurrent neural network (RNN) architectures are typically used in cases where traditional approaches might struggle due to the nature of sequential or time-series data. While traditional approaches like Hidden Markov Models (HMMs) or simple linear models can be effective in some cases, RNNs, including GRUs, offer several advantages in handling sequential data:\n",
    "\n",
    "(1). Capturing Long-Term Dependencies:\n",
    "GRUs are designed to capture long-term dependencies in sequences, which is crucial for tasks like natural language processing (NLP), speech recognition, and time-series prediction. Traditional models like HMMs might struggle to capture these long-range relationships effectively.\n",
    "\n",
    "(2). Automatic Feature Extraction:\n",
    "RNNs, including GRUs, automatically learn features from the data, eliminating the need for hand-crafted feature engineering in many cases. Traditional approaches might require manual feature selection and engineering, which can be time-consuming and limit the model's ability to adapt to changing data patterns.\n",
    "\n",
    "(3). Non-Linear Relationships:\n",
    "GRUs introduce non-linearity through activation functions like the sigmoid and tanh functions. This enables them to capture complex non-linear relationships in the data. Traditional linear models might not be as effective in capturing such relationships.\n",
    "\n",
    "(4). End-to-End Learning:\n",
    "RNNs offer end-to-end learning, meaning they can take raw input data and directly output the desired results, without requiring intermediate steps for feature extraction and transformation. This can simplify the modeling process and potentially lead to better performance.\n",
    "\n",
    "(5). Adaptability to Sequence Lengths:\n",
    "GRUs can handle variable-length sequences, which is a common scenario in many real-world applications. Traditional models might struggle with inputs of varying lengths, requiring additional preprocessing steps.\n",
    "\n",
    "(6). Scalability to Complex Tasks:\n",
    "GRUs and other advanced RNN architectures can handle more complex tasks like language translation, sentiment analysis, and music generation, where traditional approaches might fall short due to the intricate nature of the tasks.\n",
    "\n",
    "(7). Availability of Large Datasets:\n",
    "\n",
    "With the availability of large datasets, the data-driven nature of GRUs allows them to leverage the vast amounts of information to improve their performance, as opposed to traditional models that might rely more on rule-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBtlQUcxBPT9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PBQ0GyzCNMJ"
   },
   "source": [
    "GRU(Gated Recurrent Unit) :\n",
    "\n",
    "1. GRU - Gated Recurrent Unit:\n",
    "The Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) architecture that addresses the vanishing gradient problem in traditional RNNs. It was introduced by Cho et al. in 2014 as a simplified version of the long short-term memory (LSTM) architecture. GRUs have fewer parameters than LSTMs and are generally easier to train.\n",
    "\n",
    "GRUs work by using gating mechanisms to control the flow of information within the network, allowing them to capture longer-range dependencies in sequences. The key components of a GRU cell are:\n",
    "\n",
    "(a). Update Gate (z): Determines how much of the past information to keep and how much of the new input to let through.\n",
    "\n",
    "(b). Reset Gate (r): Controls the balance between relying on the previous hidden state and the current input to compute the new candidate hidden state.\n",
    "\n",
    "(c). Candidate Hidden State (h~): A temporary representation of the new hidden state that can potentially be passed to the actual hidden state.\n",
    "\n",
    "(d). Hidden State (h): The final output of the GRU cell that is used for prediction and passed to the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOKlcW1cDf2-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q5d2UTIDhHt"
   },
   "source": [
    "Mathematically, the equations for a GRU cell update are:\n",
    "\n",
    "Update Gate (z):\n",
    "z_t = sigmoid(W_z * [h_(t-1), x_t])\n",
    "\n",
    "Reset Gate (r):\n",
    "r_t = sigmoid(W_r * [h_(t-1), x_t])\n",
    "\n",
    "Candidate Hidden State (h~):\n",
    "h~t = tanh(W_h * [r_t * h(t-1), x_t])\n",
    "\n",
    "New Hidden State (h):\n",
    "h_t = (1 - z_t) * h_(t-1) + z_t * h~_t\n",
    "\n",
    "Where,\n",
    "\n",
    "z_t, r_t, and h~_t are the update gate, reset gate, and candidate hidden state at time step t, respectively.\n",
    "\n",
    "\n",
    "h_(t-1) is the previous hidden state at time step t-1.\n",
    "\n",
    "\n",
    "\n",
    "x_t is the input at time step t.\n",
    "\n",
    "\n",
    "W_z, W_r, and W_h are weight matrices for the update gate, reset gate, and candidate hidden state, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "reXUEB8UELs7",
    "outputId": "dc4302d8-093d-457a-ad57-c3d6091af9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n",
      "Epoch 1/5\n",
      "782/782 [==============================] - 703s 897ms/step - loss: 0.4646 - accuracy: 0.7752 - val_loss: 0.3891 - val_accuracy: 0.8280\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 715s 914ms/step - loss: 0.2640 - accuracy: 0.8960 - val_loss: 0.2895 - val_accuracy: 0.8809\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 714s 913ms/step - loss: 0.1765 - accuracy: 0.9349 - val_loss: 0.3096 - val_accuracy: 0.8746\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 716s 916ms/step - loss: 0.1168 - accuracy: 0.9589 - val_loss: 0.3528 - val_accuracy: 0.8800\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 710s 908ms/step - loss: 0.0739 - accuracy: 0.9756 - val_loss: 0.3825 - val_accuracy: 0.8792\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.3825 - accuracy: 0.8792\n",
      "Test score: 0.3825, Test accuracy: 0.8792\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "# Load the IMDB dataset\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Build the GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(f'Test score: {score:.4f}, Test accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5gawd-9ETEj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFlARPwvFJD6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhbwqE-WFJGZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R274VyNzFJI6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3ZOegunFJLR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1fMaFKHFJNX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrKc7fWkFJPy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exMDAKU1FJSE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3_yVmIgFJUX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4QkvzjbFJX2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
