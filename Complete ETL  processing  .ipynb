{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f686f420",
   "metadata": {},
   "source": [
    "# Web Data ETL Pipeline: \n",
    "\n",
    "A Web Data ETL (Extract, Transform, Load) pipeline is a systematic process used in data engineering to collect, transform, and load data from various sources on the internet into a structured and usable format for analysis and storage. It is essential for managing and processing large volumes of data gathered from websites, online platforms, and digital sources.\n",
    "\n",
    "\n",
    "\n",
    "The process begins with data extraction, where relevant information is collected from websites, APIs, databases, and other online sources.\n",
    "\n",
    "\n",
    "\n",
    "Then this raw data is transformed through various operations, including cleaning, filtering, structuring, and aggregating. After transformation, the data is loaded into a CSV file or database, making it accessible for further analysis, reporting, and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec994a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77bb1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saurabh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218cb7b",
   "metadata": {},
   "source": [
    "# Now let’s start by extracting text from any article on the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ae4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def extract_article_text(self):\n",
    "        response = requests.get(self.url)\n",
    "        html_content = response.content\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        article_text = soup.get_text()\n",
    "        return article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c35aab",
   "metadata": {},
   "source": [
    "# n the above code, the WebScraper class provides a way to conveniently extract the main text content of an article from a given web page URL. By creating an instance of the WebScraper class and calling its extract_article_text method, we can retrieve the textual data of the article, which can then be further processed or analyzed as needed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As we want to store the frequency of each word in the article, we need to clean and preprocess the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f807bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, nltk_stopwords):\n",
    "        self.nltk_stopwords = nltk_stopwords\n",
    "\n",
    "    def tokenize_and_clean(self, text):\n",
    "        words = text.split()\n",
    "        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
    "        return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca157b0",
   "metadata": {},
   "source": [
    "# In this code, the TextProcessor class provides a convenient way to process text data by tokenizing it into words and cleaning those words by removing non-alphabetic words and stopwords. It is often a crucial step in text analysis and natural language processing tasks. By creating an instance of the TextProcessor class and calling its tokenize_and_clean method, you can obtain a list of cleaned and filtered words from a given input text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2443f10",
   "metadata": {},
   "source": [
    "# So till now, we have defined classes for scraping and preparing the data. Now we need to define a class for the entire ETL (Extract, Transform, Load) process for extracting article text, processing it, and generating a DataFrame of word frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb589bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.nltk_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def run(self):\n",
    "        scraper = WebScraper(self.url)\n",
    "        article_text = scraper.extract_article_text()\n",
    "\n",
    "        processor = TextProcessor(self.nltk_stopwords)\n",
    "        filtered_words = processor.tokenize_and_clean(article_text)\n",
    "\n",
    "        word_freq = Counter(filtered_words)\n",
    "        df = pd.DataFrame(word_freq.items(), columns=[\"Words\", \"Frequencies\"])\n",
    "        df = df.sort_values(by=\"Frequencies\", ascending=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0cdcc6",
   "metadata": {},
   "source": [
    "# In the  code, the ETLPipeline class encapsulates the end-to-end process of extracting article text from a web page, cleaning and processing the text, calculating word frequencies, and generating a sorted DataFrame. By creating an instance of the ETLPipeline class and calling its run method, you can perform the complete ETL process and obtain a DataFrame that provides insights into the most frequently used words in the article after removing stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3313ee",
   "metadata": {},
   "source": [
    "# Now here’s how to run this pipeline to scrape textual data from any article from the web and count the frequency of each word in the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2959826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Words  Frequencies\n",
      "3           pmc            7\n",
      "100         var            5\n",
      "31      account            3\n",
      "11   government            3\n",
      "1          page            3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    article_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3543134/\"\n",
    "    \n",
    "    pipeline = ETLPipeline(article_url)\n",
    "    result_df = pipeline.run()\n",
    "    print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3736ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da72d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Words  Frequencies\n",
      "8         agent           21\n",
      "4           new           15\n",
      "39    currently           11\n",
      "103     article           11\n",
      "102  indefinite           11\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    article_url = \"https://courses.lumenlearning.com/waymaker-level1-english-gen/chapter/articles-4-5-3/\"\n",
    "    \n",
    "    \n",
    "    pipeline = ETLPipeline(article_url)\n",
    "    result_df = pipeline.run()\n",
    "    print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d90f48ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Words  Frequencies\n",
      "3           pmc            7\n",
      "100         var            5\n",
      "31      account            3\n",
      "11   government            3\n",
      "1          page            3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     article_url = \"https://courses.lumenlearning.com/waymaker-level1-english-gen/chapter/articles-4-5-3/\"\n",
    "    article_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3543134/\"\n",
    "    \n",
    "    pipeline = ETLPipeline(article_url)\n",
    "    result_df = pipeline.run()\n",
    "    print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dbefac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Frequencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>search</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>retrieve</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>content</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>follow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sure</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>box</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>websites</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sharing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>know</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>publications</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>ncbibaseurl</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>government</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>archive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>vulnerability</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Words  Frequencies\n",
       "62      additional            1\n",
       "75          search            2\n",
       "51        retrieve            1\n",
       "6          content            3\n",
       "77          follow            1\n",
       "21            sure            1\n",
       "71             box            1\n",
       "15        websites            1\n",
       "18         sharing            1\n",
       "12            know            1\n",
       "37    publications            1\n",
       "102    ncbibaseurl            2\n",
       "11      government            3\n",
       "76         archive            1\n",
       "94   vulnerability            1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.sample(15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c204648d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861aa99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
