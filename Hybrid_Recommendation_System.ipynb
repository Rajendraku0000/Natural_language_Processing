{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpH_z0eRs2Ol"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8crg2V_v1FE"
   },
   "source": [
    "A hybrid recommendation system is a recommendation technique that offers a complete and balanced approach by mixing two or more recommendation techniques. It aims to provide more accurate, diverse and personalized recommendations to users leveraging the strengths of different techniques and providing valuable user experience.\n",
    "\n",
    "What is a Hybrid Recommendation System?\n",
    "\n",
    "A hybrid recommendation system combines multiple recommendation techniques to provide more accurate and diverse recommendations to users. It uses the strengths of different approaches, such as collaborative filtering and content-based filtering, to overcome their limitations and improve the recommendation process.\n",
    "\n",
    "You must have heard of Collaborative filtering and Content-based filtering before. Collaborative filtering analyzes user-item interactions and identifies similarities between users or items to make recommendations. It recommends items users with similar preferences have liked or consumed. However, it may struggle with new or niche items having limited user interactions.\n",
    "\n",
    "On the other hand, content-based filtering focuses on features and characteristics of items to recommend similar items to users based on their preferences. It examines attributes like product descriptions, brands, categories, and user profiles. However, it may not capture the complexity of user preferences and may result in less diverse recommendations.\n",
    "\n",
    "This is where a hybrid recommendation system helps. By combining collaborative and content-based filtering in a hybrid system, we can overcome the limitations of collaborative and content-based filtering. The collaborative filtering component captures the wisdom of the crowd, while the content-based filtering component takes into account the specific features and attributes of items. This combination allows the system to provide more accurate recommendations, especially in situations where user-item interactions are rare or when personalized recommendations are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "quu-jHRwtMxt",
    "outputId": "0f826e21-c85a-4de1-dfc2-d0194ea2894d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-5e0168d6-7ca7-47b2-a150-772f8ce8bbac\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-5e0168d6-7ca7-47b2-a150-772f8ce8bbac\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fashion_products.csv to fashion_products.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "path_to_file = list(files.upload().keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWb2Z5OltP-e",
    "outputId": "12d64ed9-773f-43ab-cd1a-4aab8c30e8b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User ID  Product ID Product Name   Brand         Category  Price    Rating  \\\n",
      "0       19           1        Dress  Adidas    Men's Fashion     40  1.043159   \n",
      "1       97           2        Shoes     H&M  Women's Fashion     82  4.026416   \n",
      "2       25           3        Dress  Adidas  Women's Fashion     44  3.337938   \n",
      "3       57           4        Shoes    Zara    Men's Fashion     23  1.049523   \n",
      "4       79           5      T-shirt  Adidas    Men's Fashion     79  4.302773   \n",
      "\n",
      "    Color Size  \n",
      "0   Black   XL  \n",
      "1   Black    L  \n",
      "2  Yellow   XL  \n",
      "3   White    S  \n",
      "4   Black    M  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path_to_file)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDfTJTCawRjJ"
   },
   "source": [
    "So this data is based on fashion products for men, women, and kids. Our goal is to create two recommendation systems using collaborative and content-based filtering and then combine the recommendation techniques to build a recommendation system using a hybrid approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAFOfOa4tkq8",
    "outputId": "4f8b985d-42d1-4c55-9951-a2040b4e3a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.10.1)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=2811619 sha256=ba98a8867794ce5866dfd7085ac4f6da7e1f4b02605cee4a7e798dfe67eecae5\n",
      "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "Successfully installed scikit-surprise-1.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JJrfDywurTN"
   },
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, SVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTSHQW3Bwbmv"
   },
   "source": [
    "I have imported the Surprise library that you may not have used before. The surprise library is imported to use the SVD algorithm. SVD stands for Singular Value Decomposition. Simply put, it is a matrix factorization technique commonly used in collaborative filtering algorithms.\n",
    "\n",
    "\n",
    "First Approach: Content-Based Filtering\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now let’s move forward by creating a recommendation system using content-based filtering:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wef7yxlVu5dX",
    "outputId": "1bcc87d8-1826-4018-ab52-f1f3a663d948"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-d1fd5636659e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content_df['Content'] = content_df.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n"
     ]
    }
   ],
   "source": [
    "content_df = df[['Product ID', 'Product Name', 'Brand',\n",
    "                   'Category', 'Color', 'Size']]\n",
    "content_df['Content'] = content_df.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "\n",
    "# Use TF-IDF vectorizer to convert content into a matrix of TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "content_matrix = tfidf_vectorizer.fit_transform(content_df['Content'])\n",
    "\n",
    "content_similarity = linear_kernel(content_matrix, content_matrix)\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['User ID',\n",
    "                                  'Product ID',\n",
    "                                  'Rating']], reader)\n",
    "\n",
    "def get_content_based_recommendations(product_id, top_n):\n",
    "    index = content_df[content_df['Product ID'] == product_id].index[0]\n",
    "    similarity_scores = content_similarity[index]\n",
    "    similar_indices = similarity_scores.argsort()[::-1][1:top_n + 1]\n",
    "    recommendations = content_df.loc[similar_indices, 'Product ID'].values\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVPv457Y8SRf"
   },
   "source": [
    "(1). Create a DataFrame content_df containing relevant product information like 'Product ID', 'Product Name', 'Brand', 'Category', 'Color', and 'Size'.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(2). Add a new column 'Content' to content_df, which concatenates all non-null values from each row (product information) into a single string. This string will represent the content (features) of each product.\n",
    "\n",
    "(3). Use the TF-IDF vectorizer to convert the 'Content' column into a matrix of TF-IDF features. TF-IDF is a numerical representation that reflects the importance of words in a document relative to the entire collection of documents. Here, each product is considered a \"document,\" and the words in the 'Content' column are treated as \"terms.\"\n",
    "\n",
    "\n",
    "\n",
    "(4). Compute the content similarity matrix content_similarity. This matrix represents the similarity between each pair of products based on their TF-IDF feature representations. The higher the value in the matrix, the more similar the products are in terms of their content.\n",
    "\n",
    "\n",
    "\n",
    "(5). Prepare the Surprise dataset data for collaborative filtering. It loads the 'User ID', 'Product ID', and 'Rating' columns from the original DataFrame df into the data object.\n",
    "\n",
    "\n",
    "\n",
    "(6). Define the function get_content_based_recommendations(product_id, top_n), which takes a 'product_id' as input and returns 'top_n' product IDs as content-based recommendations.\n",
    "\n",
    "Within the function, find the index of the given 'product_id' in content_df.\n",
    "\n",
    "\n",
    "\n",
    "(7). Calculate the similarity scores between the given product and all other products using the content_similarity matrix.\n",
    "\n",
    "\n",
    "\n",
    "(8). Sort the similarity scores in descending order, excluding the given product itself (hence [1:top_n + 1]).\n",
    "\n",
    "\n",
    "\n",
    "(9). Retrieve the 'Product ID' values of the top 'top_n' similar products from content_df, and return them as recommendations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjr_mNFKwuBg"
   },
   "source": [
    "Here , we are implementing the content-based filtering component of the hybrid recommender system. We started by selecting relevant features from the dataset, including the product ID, name, brand, category, colour, and size. Then we combined these features into a single “Content” column for each product.\n",
    "\n",
    "\n",
    "Next, we used the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer to convert the content into a TF-IDF feature matrix. This matrix represents the importance of each word in the content compared to the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7w2YUbQw4cu"
   },
   "source": [
    "We then calculated the similarity between products based on their content using the cosine similarity measure. This similarity matrix captures the similarity between each pair of products based on their content.\n",
    "\n",
    "\n",
    "To get content-based recommendations, we first found the index of the target product in the similarity matrix. Then we sorted the similarity scores in descending order and selected the top N similar products. Finally, we returned the product IDs of the recommended products.\n",
    "\n",
    "\n",
    "\n",
    "Second Approach: Collaborative Filtering\n",
    "Now let’s move forward by creating a recommendation system using collaborative filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IR1w8ihIu-c4"
   },
   "outputs": [],
   "source": [
    "algo = SVD()\n",
    "trainset = data.build_full_trainset()\n",
    "algo.fit(trainset)\n",
    "\n",
    "def get_collaborative_filtering_recommendations(user_id, top_n):\n",
    "    testset = trainset.build_anti_testset()\n",
    "    testset = filter(lambda x: x[0] == user_id, testset)\n",
    "    predictions = algo.test(testset)\n",
    "    predictions.sort(key=lambda x: x.est, reverse=True)\n",
    "    recommendations = [prediction.iid for prediction in predictions[:top_n]]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "et8-dVQJ9doW"
   },
   "source": [
    "(1). Create an instance of the SVD algorithm called algo.\n",
    "\n",
    "\n",
    "\n",
    "(2). Build a full training set trainset from the dataset data. This set includes all user-item interactions (ratings) available in the dataset.\n",
    "\n",
    "\n",
    "\n",
    "(3). Fit (train) the SVD algorithm using the trainset. The algorithm learns from the existing ratings to capture user and item preferences.\n",
    "\n",
    "\n",
    "\n",
    "(4). Define the function get_collaborative_filtering_recommendations(user_id, top_n), which takes a 'user_id' as input and returns 'top_n' item IDs as collaborative filtering recommendations.\n",
    "\n",
    "\n",
    "\n",
    "(5). Create a test set testset for the user with 'user_id' by removing all items that the user has already rated from the training set. This set represents the user's unrated items.\n",
    "\n",
    "\n",
    "\n",
    "(6). Get predictions for all the items in the testset using the trained SVD algorithm algo. These predictions represent the estimated ratings the user might give to each item.\n",
    "\n",
    "\n",
    "\n",
    "(7). Sort the predictions in descending order based on the estimated ratings (est), so the items with the highest predicted ratings come first.\n",
    "\n",
    "\n",
    "\n",
    "(8). Extract the item IDs (iid) from the top 'top_n' predictions and store them in the 'recommendations' list.\n",
    "\n",
    "\n",
    "\n",
    "(9). Return the 'recommendations', which contains the item IDs of the top 'top_n' items that the user might like based on collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7HsEG7ixRT8"
   },
   "source": [
    "In the above code, we implemented the collaborative filtering component of the hybrid recommender system using the SVD (Singular Value Decomposition) algorithm.\n",
    "First, we initialized the SVD algorithm and trained it on the dataset. This step involves decomposing the user element rating matrix to capture the underlying patterns and latent factors that drive user preferences.\n",
    "To generate collaborative filtering recommendations, we then created a test set composed of user-item pairs that were not present in the training set. We have filtered this test set to only include items belonging to the target user specified by user_id.\n",
    "\n",
    "Next, we used the trained SVD model to predict the test set item ratings. These predictions represent the estimated ratings that the user would assign to the items.\n",
    "\n",
    "The predictions are then sorted by their estimated ratings in descending order. We selected the top N items with the highest estimated ratings as collaborative filtering recommendations for the user.\n",
    "\n",
    "And Finally, The Hybrid Approach\n",
    "Now let’s combine content-based and collaborative filtering methods to build a recommendation system using the Hybrid method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1_8YDcSvX8_"
   },
   "outputs": [],
   "source": [
    "def get_hybrid_recommendations(user_id, product_id, top_n):\n",
    "    content_based_recommendations = get_content_based_recommendations(product_id, top_n)\n",
    "    collaborative_filtering_recommendations = get_collaborative_filtering_recommendations(user_id, top_n)\n",
    "    hybrid_recommendations = list(set(content_based_recommendations + collaborative_filtering_recommendations))\n",
    "    return hybrid_recommendations[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGzZ_V4j-rZf"
   },
   "source": [
    "(1).  The function takes three inputs: 'user_id' (the ID of the user for whom recommendations are needed), 'product_id' (the ID of the product used for content-based recommendations), and 'top_n' (the number of top recommendations to return).\n",
    "\n",
    "\n",
    "(2).  The function first calls the get_content_based_recommendations function, passing 'product_id' as an input, to get a list of content-based recommendations for the given product. These recommendations are products similar to the given product based on their content (attributes).\n",
    "\n",
    "\n",
    "(3).  Next, the function calls the get_collaborative_filtering_recommendations function, passing 'user_id' as an input, to get a list of collaborative filtering recommendations for the given user. These recommendations are items that the user is likely to enjoy based on their past interactions with the system.\n",
    "\n",
    "\n",
    "(4).  The hybrid_recommendations list is created by combining the content-based and collaborative filtering recommendations using the + operator. The set() function is used to remove any duplicate items that may appear in both recommendation lists.\n",
    "\n",
    "\n",
    "(5).  Finally, the function returns the first 'top_n' items from the hybrid_recommendations list as the hybrid recommendations for the given user. These hybrid recommendations are a combination of content-based and collaborative filtering suggestions, aiming to provide a diverse and personalized set of top recommendations to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2KguCdPxcKo"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU2I9iogxcOG"
   },
   "source": [
    "In the above code, we combined content-based and collaborative filtering approaches to create a hybrid recommender system.\n",
    "The get_hybrid_recommendations function takes the user_id, the product_id and the desired number of top_n recommendations as input.\n",
    "\n",
    "First, it calls the get_content_based_recommendations function to retrieve a list of content-based recommendations for the specified product_id. These recommendations are based on the similarity between the characteristics of the given product and other products in the dataset.\n",
    "\n",
    "Then it calls the get_collaborative_filtering_recommendations function to get a list of collaborative filtering recommendations for the specified user_id. These recommendations are generated by leveraging historical user-item interactions and estimating user preferences based on similar user behaviours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wcIq5pjxe3m"
   },
   "source": [
    "Next, we combine the content-based and collaborative filtering recommendations by taking the union of the two lists. It ensures that hybrid recommendations include content-based and collaborative filtering recommendations based on user preferences.\n",
    "\n",
    "\n",
    "Here’s how to use our hybrid recommendation system to recommend products based on the product that a user is viewing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6K_-4N4_y-U"
   },
   "source": [
    "(1). The code sets the user_id to 6, which represents the ID of the user for whom recommendations are needed.\n",
    "\n",
    "\n",
    "(2). The product_id is set to 11, which represents the ID of the product used as a reference for content-based recommendations. The content-based recommendations will be products similar to this reference product.\n",
    "\n",
    "(3). The variable top_n is set to 10, which indicates the number of top recommendations the system should provide to the user.\n",
    "\n",
    "(4). The get_hybrid_recommendations function is called with user_id, product_id, and top_n as inputs. This function combines content-based and collaborative filtering recommendations to create hybrid recommendations for the user.\n",
    "\n",
    "(5). The recommendations variable stores the hybrid recommendations returned by the get_hybrid_recommendations function.\n",
    "\n",
    "(6). The code then prints the hybrid recommendations using a loop. For each recommendation, it prints the recommendation's position in the list (i + 1), the recommended product's ID (recommendation), and the recommendation's position ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NR6uf-YWvcOA",
    "outputId": "b3a6cbdd-8293-4528-e638-f98976de5f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommendations for User 6 based on Product 11:\n",
      "1. Product ID: 962\n",
      "1. Product ID: 962\n",
      "2. Product ID: 1188\n",
      "2. Product ID: 1188\n",
      "3. Product ID: 1104\n",
      "3. Product ID: 1104\n",
      "4. Product ID: 208\n",
      "4. Product ID: 208\n",
      "5. Product ID: 1234\n",
      "5. Product ID: 1234\n",
      "6. Product ID: 1143\n",
      "6. Product ID: 1143\n",
      "7. Product ID: 440\n",
      "7. Product ID: 440\n",
      "8. Product ID: 1113\n",
      "8. Product ID: 1113\n",
      "9. Product ID: 1147\n",
      "9. Product ID: 1147\n",
      "10. Product ID: 957\n",
      "10. Product ID: 957\n"
     ]
    }
   ],
   "source": [
    "user_id = 6\n",
    "product_id = 11\n",
    "top_n = 10\n",
    "recommendations = get_hybrid_recommendations(user_id, product_id, top_n)\n",
    "\n",
    "print(f\"Hybrid Recommendations for User {user_id} based on Product {product_id}:\")\n",
    "for i, recommendation in enumerate(recommendations):\n",
    "    print(f\"{i + 1}. Product ID: {recommendation}\")\n",
    "    print(f\"{i + 1}. Product ID: {recommendation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5M3CuVB5vjgV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JI61Ed6x7ox"
   },
   "source": [
    "Summary\n",
    "\n",
    "\n",
    "So this is how to create a hybrid recommendation system using Python. A hybrid recommendation system combines multiple recommendation techniques to provide more accurate and diverse recommendations to users. It uses the strengths of different approaches, such as collaborative filtering and content-based filtering, to overcome their limitations and improve the recommendation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59l6o6_ox86q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aA8nzg1Bymfv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d13EZpCvymiM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "encF8WQjymkm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-VVIqGOzE22"
   },
   "source": [
    "Content-based Filtering:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Content-based filtering recommends items based on their attributes or features. It uses user profiles and item attributes to identify items that are similar to the ones a user has shown interest in before.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Example: Music Recommendations\n",
    "\n",
    "\n",
    "\n",
    "Suppose we have a music recommendation system where users have rated songs and provided tags to each song. Content-based filtering will analyze the tags and attributes of songs a user has liked to recommend similar songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfiK_bUcymoO",
    "outputId": "3a5e9ad3-5ca7-4bd0-cc01-aa322d7abd7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for 'Song A':\n",
      "1    Song B\n",
      "2    Song C\n",
      "3    Song D\n",
      "Name: Song, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Sample data representing songs and their tags\n",
    "data = {\n",
    "    'Song': ['Song A', 'Song B', 'Song C', 'Song D', 'Song E'],\n",
    "    'Tags': ['Pop, Dance', 'Pop, Rock', 'Rock', 'Jazz', 'Jazz, Blues']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a TF-IDF vectorizer to convert tags into numerical vectors\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Tags'])\n",
    "\n",
    "# Compute the cosine similarity of the TF-IDF matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Function to get song recommendations based on similarity\n",
    "def get_recommendations(song_name, cosine_sim=cosine_sim):\n",
    "    idx = df[df['Song'] == song_name].index[0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:4]  # Get top 3 similar songs (excluding the input song)\n",
    "    song_indices = [i[0] for i in sim_scores]\n",
    "    return df['Song'].iloc[song_indices]\n",
    "\n",
    "# Get recommendations for a song\n",
    "input_song = 'Song A'\n",
    "recommendations = get_recommendations(input_song)\n",
    "print(f\"Recommendations for '{input_song}':\")\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGKicn_oAmCm"
   },
   "source": [
    "(1). The code imports the necessary libraries, including Pandas for data handling, and the TF-IDF vectorizer and linear_kernel from scikit-learn for text feature extraction and similarity computation.\n",
    "\n",
    "\n",
    "(2). A sample dataset is provided, representing songs and their associated tags. The 'Song' column contains the song names, and the 'Tags' column contains the tags describing the genre or characteristics of each song.\n",
    "\n",
    "\n",
    "(3). A DataFrame 'df' is created using the sample data, representing the songs and their tags.\n",
    "\n",
    "\n",
    "(4). A TF-IDF vectorizer 'tfidf_vectorizer' is initialized to convert the tags into numerical vectors. The TF-IDF technique calculates the importance of each tag relative to the entire collection of songs.\n",
    "\n",
    "\n",
    "(5). The TF-IDF matrix 'tfidf_matrix' is computed by fitting and transforming the tag data from the 'df' DataFrame using the 'tfidf_vectorizer'.\n",
    "\n",
    "\n",
    "(6). The cosine similarity matrix 'cosine_sim' is computed based on the 'tfidf_matrix'. The cosine similarity measures how similar two songs are based on their tag vectors.\n",
    "\n",
    "\n",
    "(7). A function 'get_recommendations(song_name, cosine_sim=cosine_sim)' is defined to provide song recommendations based on similarity. It takes the 'song_name' as input and the 'cosine_sim' matrix (defaulted to the precomputed similarity matrix).\n",
    "\n",
    "\n",
    "(8). Inside the function, the index 'idx' of the input 'song_name' in the 'df' DataFrame is determined. This index corresponds to the song's position in the DataFrame.\n",
    "\n",
    "\n",
    "(9). The similarity scores between the input song and all other songs are obtained from the 'cosine_sim' matrix. These scores are sorted in descending order to get the most similar songs first.\n",
    "\n",
    "\n",
    "(10). The function returns the names of the top 3 most similar songs as recommendations (excluding the input song) by accessing their indices in the DataFrame and retrieving their 'Song' names.\n",
    "\n",
    "\n",
    "(11). The code then uses the 'get_recommendations' function to get recommendations for the song 'Song A' and prints the results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVOMejm-ynSq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUD9J25xy0A7"
   },
   "source": [
    "Example - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZX-KMc-y1AD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEZHNtOjzXEO"
   },
   "source": [
    "Collaborative Filtering:\n",
    "\n",
    "\n",
    "Collaborative filtering is based on the idea that users who have agreed in the past will agree in the future. It makes recommendations by finding similarities between users or items based on their interactions with the system.\n",
    "\n",
    "\n",
    "There are two main types of collaborative filtering:\n",
    "\n",
    "a. User-based Collaborative Filtering: It recommends items to a target user based on the preferences of other users who are similar to the target user.\n",
    "\n",
    "\n",
    "\n",
    "b. Item-based Collaborative Filtering: It recommends items to a target user based on the preferences of other users who\n",
    "\n",
    "have shown similarities in their interactions with items.\n",
    "\n",
    "Example: Movie Recommendations\n",
    "\n",
    "\n",
    "\n",
    "Suppose we have a movie recommendation system where users rate movies on a scale of 1 to 5. Collaborative filtering will analyze the ratings of users and recommend movies based on similarities between users' preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAvnRNWtzgX8",
    "outputId": "1c164737-34bb-4e8f-bdb2-8a2ac6221810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ml-100k could not be found. Do you want to download it? [Y/n] y\n",
      "Trying to download dataset from https://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
      "Done! Dataset ml-100k has been saved to /root/.surprise_data/ml-100k\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0197\n",
      "RMSE: 1.0197130892940214\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse\n",
    "\n",
    "# Load data\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Use user-based collaborative filtering\n",
    "sim_options = {\n",
    "    'name': 'cosine',    # Use cosine similarity\n",
    "    'user_based': True   # Use user-based collaborative filtering\n",
    "}\n",
    "algo = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Predict ratings for testset\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = rmse(predictions)\n",
    "print(f'RMSE: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEnfooll05oW",
    "outputId": "e8e9fa11-70fb-4d96-959f-3b395c7d45b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Top 10 movie recommendations for user 2:\n",
      "333\n",
      "498\n",
      "41\n",
      "356\n",
      "423\n",
      "77\n",
      "176\n",
      "217\n",
      "174\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse\n",
    "\n",
    "# Load your custom dataset or use ml-100k\n",
    "# Modify the path or create a custom dataset based on your data\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Split the data into training and testing sets (modify test_size as needed)\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Use user-based collaborative filtering\n",
    "sim_options = {\n",
    "    'name': 'cosine',    # Use cosine similarity\n",
    "    'user_based': True   # Use user-based collaborative filtering\n",
    "}\n",
    "algo = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Function to get movie recommendations for a user\n",
    "def get_movie_recommendations(user_id, algo, num_recommendations=5):\n",
    "    # Check if the provided user ID exists in the training set\n",
    "    if user_id in trainset._raw2inner_id_users:\n",
    "        user_inner_id = trainset.to_inner_uid(user_id)\n",
    "        user_movies = set([movie_id for movie_id, _ in trainset.ur[user_inner_id]])\n",
    "        all_movies = set(trainset.all_items())\n",
    "        remaining_movies = list(all_movies - user_movies)\n",
    "        predictions = [algo.predict(user_inner_id, movie_id) for movie_id in remaining_movies]\n",
    "        recommendations = sorted(predictions, key=lambda x: x.est, reverse=True)[:num_recommendations]\n",
    "        return [trainset.to_raw_iid(prediction.iid) for prediction in recommendations]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Example: Get movie recommendations for a specific user (change 'user_id' accordingly)\n",
    "user_id = '2'  # Replace '1' with the user ID for which you want movie recommendations\n",
    "\n",
    "# Get movie recommendations for the given user\n",
    "num_recommendations = 10  # Change the number of recommendations as needed\n",
    "recommendations = get_movie_recommendations(user_id, algo, num_recommendations)\n",
    "\n",
    "# Print movie recommendations\n",
    "if recommendations:\n",
    "    print(f\"Top {num_recommendations} movie recommendations for user {user_id}:\")\n",
    "    for movie_id in recommendations:\n",
    "        print(movie_id)\n",
    "else:\n",
    "    print(f\"User {user_id} does not exist in the training data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0aiFVpzB5R7"
   },
   "source": [
    "(1). The code imports the necessary libraries, including Surprise for collaborative filtering, and the train_test_split function for splitting the dataset into training and testing sets.\n",
    "\n",
    "\n",
    "\n",
    "(2). The Surprise library has a built-in dataset ('ml-100k') that contains movie ratings by users. If you want to use your own dataset, you can modify the 'data' variable to load your custom dataset.\n",
    "\n",
    "\n",
    "\n",
    "(3). The dataset is split into training and testing sets using the train_test_split function. This is a common practice to evaluate the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "(4). The recommendation model is initialized with user-based collaborative filtering using the KNNBasic algorithm with cosine similarity. The KNNBasic algorithm looks for similar users to make recommendations.\n",
    "\n",
    "\n",
    "\n",
    "(5).  The model is trained on the training set using the fit method.\n",
    "\n",
    "\n",
    "\n",
    "(6). A function 'get_movie_recommendations' is defined to get movie recommendations for a specific user.\n",
    "\n",
    "\n",
    "\n",
    "(7) . The function checks if the provided 'user_id' exists in the training set. If not, it returns an empty list since the user is not present in the data.\n",
    "\n",
    "\n",
    "\n",
    "(8).  If the user exists in the training set, the function finds the internal user ID using 'trainset.to_inner_uid(user_id)' and retrieves the movies the user has already rated.\n",
    "\n",
    "\n",
    "\n",
    "(9).  The function then creates a list of remaining movies that the user has not rated.\n",
    "\n",
    "\n",
    "\n",
    "(10).  For each remaining movie, the function predicts the user's rating using the collaborative filtering model (KNNBasic) and stores the predictions.\n",
    "\n",
    "\n",
    "\n",
    "(11).  The predictions are sorted based on the estimated ratings (est) in descending order, and the top 'num_recommendations' movies are selected as recommendations.\n",
    "\n",
    "\n",
    "\n",
    "(12).   The function returns the 'num_recommendations' movie IDs as recommendations for the given user.\n",
    "\n",
    "\n",
    "\n",
    "(13).  The code provides an example by getting movie recommendations for 'user_id = '2'' and printing the recommendations to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7IyCplp1gmf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
